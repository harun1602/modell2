{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T22:26:12.754951Z",
     "start_time": "2025-06-01T22:14:22.278427Z"
    }
   },
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyod.models.anogan import AnoGAN\n",
    "\n",
    "# daten laden\n",
    "normal_path = \"transients/S01/1/Normal/q_data\"\n",
    "relay_path  = \"transients/S01/1/Relay/q_data\"\n",
    "\n",
    "with h5py.File(\"datasets/transients_cleaned_padded.h5\", \"r\") as f:\n",
    "    normal_group = f[normal_path]\n",
    "    relay_group  = f[relay_path]\n",
    "\n",
    "    n_norm = normal_group[\"0\"].shape[0]\n",
    "    n_relay = relay_group[\"0\"].shape[0]\n",
    "\n",
    "    dataset_normal = np.zeros((49998, n_norm))\n",
    "    dataset_relay  = np.zeros((49666, n_relay))\n",
    "\n",
    "    for i in range(49998):\n",
    "        dataset_normal[i] = normal_group[str(i)][:]\n",
    "    for i in range(49666):\n",
    "        try:\n",
    "            dataset_relay[i] = relay_group[str(i)][:]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "# daten definieren und skalieren\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train               = dataset_normal[:47500]\n",
    "X_test_normal         = dataset_normal[47500:]      # 2498 Normal\n",
    "X_test_relay          = dataset_relay[47500:]        # 2166 Relay\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled        = scaler.transform(X_train)\n",
    "X_test_norm_scaled    = scaler.transform(X_test_normal)\n",
    "X_test_relay_scaled   = scaler.transform(X_test_relay)\n",
    "\n",
    "X_test_combined = np.vstack((X_test_normal, X_test_relay))\n",
    "X_test_combined_scaled = np.vstack((X_test_norm_scaled, X_test_relay_scaled))\n",
    "\n",
    "y_true = np.array([0] * len(X_test_normal) + [1] * len(X_test_relay))\n",
    "\n",
    "# modell trainieren, contamination gibt an, wie viele outlier man im trainigsdatensatz hat\n",
    "# contamination kann man bei reiner novelty detection zb auf 0.01 oder 0.05 setzen\n",
    "for l in {2, 8, 32}:\n",
    "    for d in {0.1, 0.2, 0.3}:\n",
    "        for e in {10, 20, 50}:\n",
    "            clf = AnoGAN(contamination=len(X_test_relay)/len(X_test_combined), latent_dim_G=l, dropout_rate=d, epochs_query=e)\n",
    "            clf.fit(X_train_scaled)\n",
    "\n",
    "            # pyod.predict: 0 = inlier, 1 = outlier\n",
    "            y_pred = clf.predict(X_test_combined_scaled)\n",
    "\n",
    "            # cm und report\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "            print(f\"latent {l}\")\n",
    "            print(f\"dropout {d}\")\n",
    "            print(f\"epoch {e}\")\n",
    "            print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=[\"Normal\",\"Relay\"]))\n",
    "\n",
    "            # plot\n",
    "            plt.figure(figsize=(6,5))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "                        xticklabels=[\"Relay\",\"Normal\"],\n",
    "                        yticklabels=[\"Relay\",\"Normal\"],\n",
    "                        cbar=False)\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"True\")\n",
    "            plt.title(\"Konfusionsmatrix\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 55\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m50\u001B[39m}:\n\u001B[1;32m     54\u001B[0m     clf \u001B[38;5;241m=\u001B[39m AnoGAN(contamination\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(X_test_relay)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(X_test_combined), latent_dim_G\u001B[38;5;241m=\u001B[39ml, dropout_rate\u001B[38;5;241m=\u001B[39md, epochs_query\u001B[38;5;241m=\u001B[39me)\n\u001B[0;32m---> 55\u001B[0m     \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# pyod.predict: 0 = inlier, 1 = outlier\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mpredict(X_test_combined_scaled)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyod/models/anogan.py:324\u001B[0m, in \u001B[0;36mAnoGAN.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    322\u001B[0m loss_D \u001B[38;5;241m=\u001B[39m loss_D_real \u001B[38;5;241m+\u001B[39m loss_D_fake\n\u001B[1;32m    323\u001B[0m optimizer_d\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 324\u001B[0m \u001B[43mloss_D\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m optimizer_d\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    327\u001B[0m fake_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscriminator(generated_data)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
