{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T20:26:45.959736Z",
     "start_time": "2025-06-02T20:02:21.762632Z"
    }
   },
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyod.models.loci import LOCI\n",
    "\n",
    "# daten laden\n",
    "normal_path = \"transients/S01/1/Normal/q_data\"\n",
    "relay_path = \"transients/S01/1/Relay/q_data\"\n",
    "\n",
    "with h5py.File('datasets/transients_cleaned_padded.h5', 'r') as f:\n",
    "    normal_group = f[normal_path]\n",
    "    relay_group = f[relay_path]\n",
    "\n",
    "    n_norm = normal_group[\"0\"].shape[0]\n",
    "    n_relay = relay_group[\"0\"].shape[0]\n",
    "\n",
    "    dataset_normal = np.zeros((49998, n_norm))\n",
    "    dataset_relay = np.zeros((49666, n_relay))\n",
    "\n",
    "    for i in range(49998):\n",
    "        dataset_normal[i] = normal_group[str(i)][:]\n",
    "    for i in range(49666):\n",
    "        try:\n",
    "            dataset_relay[i] = relay_group[str(i)][:]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "# daten definieren und skalieren\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = dataset_normal[:47500]\n",
    "X_test_normal = dataset_normal[47500:]  # 4998 Normal\n",
    "X_test_relay = dataset_relay[47500:]  # 4998 Relay\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_norm_scaled = scaler.transform(X_test_normal)\n",
    "X_test_relay_scaled = scaler.transform(X_test_relay)\n",
    "\n",
    "X_test_combined = np.vstack((X_test_normal, X_test_relay))\n",
    "X_test_combined_scaled = np.vstack((X_test_norm_scaled, X_test_relay_scaled))\n",
    "\n",
    "y_true = np.array([0] * len(X_test_normal) + [1] * len(X_test_relay))\n",
    "\n",
    "print(len(X_test_relay) / len(X_test_combined))\n",
    "# modell trainieren, contamination gibt an, wie viele outlier man im trainigsdatensatz hat\n",
    "# contamination kann man bei reiner novelty detection zb auf 0.01 oder 0.05 setzen\n",
    "for a in {0.2, 0.5, 0.8}:\n",
    "    for k in {1, 3, 5}:\n",
    "        clf = LOCI(contamination=0.01, alpha=0.5, k=3)\n",
    "        clf.fit(X_train)\n",
    "\n",
    "        # pyod.predict: 0 = inlier, 1 = outlier\n",
    "        y_pred = clf.predict(X_test_combined)\n",
    "\n",
    "        # cm und report\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "        print(a)\n",
    "        print(k)\n",
    "        print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=[\"Normal\", \"Relay\"]))\n",
    "\n",
    "        # plot\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "                    xticklabels=[\"Relay\", \"Normal\"],\n",
    "                    yticklabels=[\"Relay\", \"Normal\"],\n",
    "                    cbar=False)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Konfusionsmatrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4644082332761578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 55\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m5\u001B[39m}:\n\u001B[1;32m     54\u001B[0m     clf \u001B[38;5;241m=\u001B[39m LOCI(contamination\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m---> 55\u001B[0m     \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# pyod.predict: 0 = inlier, 1 = outlier\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mpredict(X_test_combined)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyod/models/loci.py:233\u001B[0m, in \u001B[0;36mLOCI.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    231\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(X)\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_n_classes(y)\n\u001B[0;32m--> 233\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecision_scores_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calculate_decision_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels_ \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecision_scores_ \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthreshold_)\u001B[38;5;241m.\u001B[39mastype(\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mravel()\n\u001B[1;32m    237\u001B[0m \u001B[38;5;66;03m# calculate for predict_proba()\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyod/models/loci.py:202\u001B[0m, in \u001B[0;36mLOCI._calculate_decision_score\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    199\u001B[0m critical_values \u001B[38;5;241m=\u001B[39m _get_critical_values(dist_matrix, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha,\n\u001B[1;32m    200\u001B[0m                                        p_ix, r_max)\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m critical_values:\n\u001B[0;32m--> 202\u001B[0m     n_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_alpha_n\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdist_matrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43m_get_sampling_N\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdist_matrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m                                                 \u001B[49m\u001B[43mp_ix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     cur_alpha_n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_alpha_n(dist_matrix, p_ix, r)\n\u001B[1;32m    206\u001B[0m     n_hat \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(n_values)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyod/models/loci.py:176\u001B[0m, in \u001B[0;36mLOCI._get_alpha_n\u001B[0;34m(self, dist_matrix, indices, r)\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m alpha_n\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     alpha_n \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount_nonzero\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdist_matrix\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m alpha_n\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/_core/numeric.py:456\u001B[0m, in \u001B[0;36m_count_nonzero_dispatcher\u001B[0;34m(a, axis, keepdims)\u001B[0m\n\u001B[1;32m    452\u001B[0m     multiarray\u001B[38;5;241m.\u001B[39mcopyto(res, fill_value, casting\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munsafe\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_count_nonzero_dispatcher\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (a,)\n\u001B[1;32m    460\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_count_nonzero_dispatcher)\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcount_nonzero\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyod.models.loci import LOCI\n",
    "\n",
    "# daten laden\n",
    "normal_path = \"transients/S01/1/Normal/q_data\"\n",
    "relay_path = \"transients/S01/1/Relay/q_data\"\n",
    "\n",
    "with h5py.File('datasets/transients_cleaned_padded.h5', 'r') as f:\n",
    "    normal_group = f[normal_path]\n",
    "    relay_group = f[relay_path]\n",
    "\n",
    "    n_norm = normal_group[\"0\"].shape[0]\n",
    "    n_relay = relay_group[\"0\"].shape[0]\n",
    "\n",
    "    dataset_normal = np.zeros((49998, n_norm))\n",
    "    dataset_relay = np.zeros((49666, n_relay))\n",
    "\n",
    "    for i in range(49998):\n",
    "        dataset_normal[i] = normal_group[str(i)][:]\n",
    "    for i in range(49666):\n",
    "        try:\n",
    "            dataset_relay[i] = relay_group[str(i)][:]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "# daten definieren und skalieren\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = dataset_normal[:47500]\n",
    "X_test_normal = dataset_normal[47500:]  # 4998 Normal\n",
    "X_test_relay = dataset_relay[47500:]  # 4998 Relay\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_norm_scaled = scaler.transform(X_test_normal)\n",
    "X_test_relay_scaled = scaler.transform(X_test_relay)\n",
    "\n",
    "X_test_combined = np.vstack((X_test_normal, X_test_relay))\n",
    "X_test_combined_scaled = np.vstack((X_test_norm_scaled, X_test_relay_scaled))\n",
    "\n",
    "y_true = np.array([0] * len(X_test_normal) + [1] * len(X_test_relay))\n",
    "\n",
    "print(len(X_test_relay) / len(X_test_combined))\n",
    "# modell trainieren, contamination gibt an, wie viele outlier man im trainigsdatensatz hat\n",
    "# contamination kann man bei reiner novelty detection zb auf 0.01 oder 0.05 setzen\n",
    "for a in {0.2, 0.5, 0.8}:\n",
    "    for k in {1, 3, 5}:\n",
    "        clf = LOCI(contamination=0.01, alpha=0.5, k=3)\n",
    "        clf.fit(X_train_scaled)\n",
    "\n",
    "        # pyod.predict: 0 = inlier, 1 = outlier\n",
    "        y_pred = clf.predict(X_test_combined_scaled)\n",
    "\n",
    "        # cm und report\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "        print(a)\n",
    "        print(k)\n",
    "        print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=[\"Normal\", \"Relay\"]))\n",
    "\n",
    "        # plot\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "                    xticklabels=[\"Relay\", \"Normal\"],\n",
    "                    yticklabels=[\"Relay\", \"Normal\"],\n",
    "                    cbar=False)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Konfusionsmatrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "b6c4f8a501fef4b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
